{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f4833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/rv/Projects/7CS074') # Change to the project root directory\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import global_vars \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from evaluation import select_best_model_cv\n",
    "from features import engineer_features\n",
    "from models import get_type_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b05130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will check and run the preprocessing script\n",
    "# In case this is already done, this will not overwrite existing files\n",
    "from preprocessing import process_raw_multiple_data_files\n",
    "\n",
    "process_raw_multiple_data_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d45b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(global_vars.DATASET_CLEAN_FILE_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {global_vars.DATASET_CLEAN_FILE_PATH}. Please ensure the dataset is placed correctly.\")\n",
    "\n",
    "df = pd.read_csv(global_vars.DATASET_CLEAN_FILE_PATH, sep=',', engine='python') # read with proper delimiter handling, and with python engine always\n",
    "if df.empty:\n",
    "    raise ValueError(\"Loaded dataset is empty. Please check the dataset file.\")\n",
    "\n",
    "print(f\"Data loaded successfully from {global_vars.DATASET_CLEAN_FILE_PATH}.\")\n",
    "\n",
    "target_col = 'price'\n",
    "\n",
    "min_samples_per_make=300\n",
    "cv_splits=5\n",
    "\n",
    "trained_models_per_make = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_models = get_type_models('regression')\n",
    "\n",
    "Y_global = df[target_col]\n",
    "X_global = engineer_features(\n",
    "    df,\n",
    "    Y_global,\n",
    "    global_vars.LOW_CATEGORICAL_FEATURES_OVERALL,\n",
    "    global_vars.HIGH_CATEGORICAL_FEATURES_GLOBAL\n",
    ")\n",
    "\n",
    "global_X_train, global_X_test, global_y_train, global_y_test = train_test_split(\n",
    "    X_global, Y_global, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "global_best_name, global_best_model, global_cv_scores = select_best_model_cv(\n",
    "    global_X_train, global_y_train, candidate_models, cv_splits\n",
    ")\n",
    "\n",
    "global_predictions = global_best_model.predict(global_X_test)\n",
    "global_feature_names = global_X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc58f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Per-make models\n",
    "# If the length of each dataframe of group is lower than the minimum sample variable, we shall pass and not create a 'per-make' model\n",
    "# This is to make sure that algorithms like 'Random Forrest' gets trained on larger sets of data, as indented, if the condition is true, the make would fallback into the global model above.\n",
    "for make, group_df in df.groupby(\"make\"):\n",
    "    if len(group_df) < min_samples_per_make:\n",
    "        continue\n",
    "    \n",
    "    Y = group_df[target_col]\n",
    "    X = engineer_features(\n",
    "        group_df, \n",
    "        Y,\n",
    "        global_vars.LOW_CATEGORICAL_FEATURES_OVERALL,\n",
    "        global_vars.HIGH_CATEGORICAL_FEATURES_PER_MAKE\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, Y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    best_name, best_model, cv_scores = select_best_model_cv(\n",
    "        X_train, y_train, candidate_models, cv_splits\n",
    "    )\n",
    "\n",
    "    predictions = best_model.predict(X_test)\n",
    "    trained_models_per_make[make] = {\n",
    "        \"model\": best_model,\n",
    "        \"feature_names\": X_train.columns,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "        \"predictions\": predictions,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f39f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import regression_metrics\n",
    "\n",
    "global_metrics = regression_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n Overall Dataset:\")\n",
    "print(f\"  Best Model: {global_best_name}\")\n",
    "print(f\"  MAE: £{global_metrics['MAE']:.2f}\")\n",
    "print(f\"  RMSE: £{global_metrics['RMSE']:.2f}\")\n",
    "print(f\"  R²: {global_metrics['R2']:.3f}\")\n",
    "print(f\"  MAPE: {global_metrics['MAPE']:.2f}%\")\n",
    "\n",
    "for make_vehicle, data in trained_models_per_make.items():\n",
    "\ty_test = data['y_test']\n",
    "\tpredictions = data['predictions']\n",
    "\tmetrics = regression_metrics(y_test, predictions)\n",
    "\n",
    "\tprint(f\"\\n{make_vehicle}:\")\n",
    "\tprint(f\"  Best Model: {data['model']}\")\n",
    "\tprint(f\"  MAE: £{metrics['MAE']:.2f}\")\n",
    "\tprint(f\"  RMSE: £{metrics['RMSE']:.2f}\")\n",
    "\tprint(f\"  R²: {metrics['R2']:.3f}\")\n",
    "\tprint(f\"  MAPE: {metrics['MAPE']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baa05cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from features import get_feature_effects\n",
    "from visualisation import plot_actual_vs_predicted, plot_feature_importances, plot_price_vs_milage, plot_residuals\n",
    "\n",
    "values, kind = get_feature_effects(global_best_model)\n",
    "\n",
    "plot_feature_importances(\n",
    "\tvalues,\n",
    "\tglobal_feature_names,\n",
    "\tf\"{kind.title()}s - Overall\"\n",
    ")\n",
    "\n",
    "# Actual vs Predicted\n",
    "plot_actual_vs_predicted(\n",
    "\ty_test, \n",
    "\tpredictions, \n",
    "\tf'Random Forest Regressor - Overall', \n",
    "\tmetrics['R2']\n",
    ")\n",
    "\n",
    "# Residuals plot\n",
    "plot_residuals(\n",
    "\ty_test, \n",
    "\tpredictions, \n",
    "\tf'Random Forest - Overall'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0575f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for make_vehicle, data in trained_models_per_make.items():\n",
    "\tmodel = data[\"model\"]\n",
    "\ty_test = data['y_test']\n",
    "\tfeature_names = data[\"feature_names\"]\n",
    "\tpredictions = data['predictions']\n",
    "\tmetrics = regression_metrics(y_test, predictions)\n",
    "\n",
    "\tvalues, kind = get_feature_effects(model)\n",
    "\n",
    "\tn = model.n_features_in_\n",
    "\tfeature_names = feature_names[:n]\n",
    "\tvalues = values[:n]\n",
    " \n",
    "\tplot_feature_importances(\n",
    "\t\tvalues,\n",
    "\t\tfeature_names,\n",
    "\t\tf\"{kind.title()}s - {make_vehicle}\"\n",
    "\t)\n",
    "\n",
    "\t# Actual vs Predicted\n",
    "\tplot_actual_vs_predicted(\n",
    "\t\ty_test, \n",
    "\t\tpredictions, \n",
    "\t\tf'Random Forest Regressor - {make_vehicle}', \n",
    "\t\tmetrics['R2']\n",
    "\t)\n",
    "\n",
    "\t# Residuals plot\n",
    "\tplot_residuals(\n",
    "\t\ty_test, \n",
    "\t\tpredictions, \n",
    "\t\tf'Random Forest - {make_vehicle}'\n",
    "\t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
